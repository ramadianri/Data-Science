{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Description:\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\n[Home Credit](http://www.homecredit.net/about-us.aspx) strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries and The Data\nFirst, we import necessary libraries, such as:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, import the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load train and test dataset\ntrain = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\ntest = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\n\nprevious_application = pd.read_csv('/kaggle/input/home-credit-default-risk/previous_application.csv')\ninstallment_payment = pd.read_csv('/kaggle/input/home-credit-default-risk/installments_payments.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n## Exploration: Train and Test Dataset\n### - Concise Summary\nDisplay ```info()``` and ```head()``` to familiarize ourself with the train and test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#concise summary of train dataset\ntrain.info()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concise summary of test dataset\ntest.info()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable defines if the client had payment difficulties, marked as 1, meaning the client with late payment more than X days, while other all other cases marked as 0."},{"metadata":{},"cell_type":"markdown","source":"### - Check For Anomalies\nOne way to do this is by analyze at the output of ```describe()``` method. We will check for anomalies such as typo, extreme outliers, dtype error between numerical and categorical, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe dataset\ntrain.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick observation on the ```describe()``` output :\n\n- The maximum value of DAYS_EMPLOYED feature is a positive value. That seems an value error, since DAYS_EMPLOYED feature description is 'How many days before the application the person started current employment', and supposed to be a negative value. Let's plot ditribution of DAYS_EMPLOYED feature to visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot distribution\nsns.distplot(train['DAYS_EMPLOYED']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above, we can see there are quite a lot outliers with values = 365243. Since we don't have any information whether it was a typo or on purpose, so we will handling it by set this anomalies to a missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#set anomalies to a missing value\ntrain['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace=True)\ntest['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot distribution after removing anomalies\nsns.distplot(train['DAYS_EMPLOYED']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the distribution looks like what we would expect."},{"metadata":{},"cell_type":"markdown","source":"### - Check For Duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for duplicated data\nprint('Duplicated value(s) on the train dataset : ', train.duplicated().sum())\nprint('Duplicated value(s) on the test dataset  : ', test.duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no duplicated data."},{"metadata":{},"cell_type":"markdown","source":"### - Check The Distribution of The Target Array"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot distribution of the target array\nsns.countplot(train['TARGET']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of the target array distribution:')\nprint('--------------------------------------------')\nprint(train['TARGET'].value_counts() / len(train['TARGET']) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the distribution plot, we can see the dataset is highly unbalanced, the positive target account for 8.07% of all target. To deal with imbalanced data, we can either using a resampling technique such as over- or under-sampling or set ```class_weight``` 'to balanced' when tuning the machine learning model. In this case, we will use ```class_weight```, because resampling technique tends to overfitting when data is highly unbalanced."},{"metadata":{},"cell_type":"markdown","source":"### - Check For Missing Values\nCheck for percentage of missing values in each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('percentage of missing values for each feature:')\nprint('----------------------------------------------')\ntrain.isnull().sum().sort_values(ascending=False) / len(train) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the strategy where we will fill missing values in categorical features with its ```mode()``` and fill missing values in numerical features with its ```mean()```."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in test.columns:\n    if (train[feature].dtype == 'object'):\n        #fill missing values in categorical features with its mode() \n        train[feature].fillna(train[feature].mode()[0], inplace=True)\n        test[feature].fillna(test[feature].mode()[0], inplace=True)\n    else:\n        #fill missing values in numerical features with its mean()\n        train[feature].fillna(train[feature].mean(), inplace=True)\n        test[feature].fillna(test[feature].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for any missing data\nprint('missing data in the train dataset : ', train.isnull().any().sum())\nprint('missing data in the test dataset : ', test.isnull().any().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration: Previous Application Dataset\n### - Concise Summary\nDisplay ```info()``` and ```head()``` to familiarize ourself with previous application dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#concise summary of train dataset\nprevious_application.info()\nprevious_application.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Check For Anomalies"},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe dataset\nprevious_application.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick observation on the ```describe()``` output :\n\n- AMT_DOWN_PAYMENT has negative value. Let's take a deeper look at the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_application[previous_application['AMT_DOWN_PAYMENT'] < 0]['AMT_DOWN_PAYMENT'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are only 2 negative values, we will set them to 0 since it will not significantly affect the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"#set negative value to 0\nprevious_application.loc[previous_application['AMT_DOWN_PAYMENT'] < 0 , 'AMT_DOWN_PAYMENT'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- DAYS_FIRST_DRAWING, DAYS_FIRST_DUE, DAYS_LAST_DUE_1ST_VERSION, DAYS_LAST_DUE and DAYS_TERMINATION features have a positive value. That seems an value error, since those features description are 'Relative to application date of current application when ... ', and supposed to be a negative value. Let's plot ditribution of those features to visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_FIRST_DRAWING'], kde=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_FIRST_DUE'], kde=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_LAST_DUE_1ST_VERSION'], kde=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_LAST_DUE'], kde=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_TERMINATION'], kde=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same as before, Since we don't have any information whether it was a typo or on purpose, so we will handling it by set this anomalies to a missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#set anomalies to a missing value\nprevious_application['DAYS_FIRST_DRAWING'].replace({365243 : np.nan}, inplace=True)\nprevious_application['DAYS_FIRST_DUE'].replace({365243 : np.nan}, inplace=True)\nprevious_application['DAYS_LAST_DUE_1ST_VERSION'].replace({365243 : np.nan}, inplace=True)\nprevious_application['DAYS_LAST_DUE'].replace({365243 : np.nan}, inplace=True)\nprevious_application['DAYS_TERMINATION'].replace({365243 : np.nan}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plot distribution after removing anomalies."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_FIRST_DRAWING']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_FIRST_DUE']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_LAST_DUE_1ST_VERSION']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_LAST_DUE']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(previous_application['DAYS_TERMINATION']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Check For Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('percentage of missing values for each feature:')\nprint('----------------------------------------------')\nprevious_application.isnull().sum().sort_values(ascending=False) / len(previous_application) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in previous_application.columns:\n    if (previous_application[feature].dtype == 'object'):\n        #fill missing values in categorical features with its mode() \n        previous_application[feature].fillna(previous_application[feature].mode()[0], inplace=True)\n    else:\n        #fill missing values in numerical features with its mean() \n        previous_application[feature].fillna(previous_application[feature].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for any missing data\nprint('missing data in previous application dataset : ', previous_application.isnull().any().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration: Installment Payment\n### - Concise Summary\nDisplay ```info()``` and ```head()``` to familiarize ourself with installment payment dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#concise summary\ninstallment_payment.info()\ninstallment_payment.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Check For Anomalies"},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe dataset\ninstallment_payment.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No anomalies were found."},{"metadata":{},"cell_type":"markdown","source":"### - Check For Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('percentage of missing values for each feature:')\nprint('----------------------------------------------')\ninstallment_payment.isnull().sum().sort_values(ascending=False) / len(installment_payment) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in installment_payment.columns:\n    if (installment_payment[feature].dtype == 'object'):\n        #fill missing values in categorical features with its mode() \n        installment_payment[feature].fillna(installment_payment[feature].mode()[0], inplace=True)\n    else:\n        #fill missing values in numerical features with its mean() \n        installment_payment[feature].fillna(installment_payment[feature].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for any missing data\nprint('missing data in installment payment dataset : ', installment_payment.isnull().any().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n## Previous Application Dataset\n### - Feature Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for each ID, count the number of previous application\nprev_app_count = previous_application[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR', as_index=False).count()\nprev_app_count.rename(columns={'SK_ID_PREV':'PREV_APP_COUNT'}, inplace=True)\n\n#merge to train and test dataset\ntrain = pd.merge(train, prev_app_count, on='SK_ID_CURR')\ntest = pd.merge(test, prev_app_count, on='SK_ID_CURR')\n\nprev_app_count.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recent application for each ID\nrecent_app = previous_application[['SK_ID_CURR', 'DAYS_DECISION']].groupby('SK_ID_CURR', as_index=False).max()\nrecent_app.rename(columns={'DAYS_DECISION':'RECENT_APP'}, inplace=True)\n\n#merge to train and test dataset\ntrain = pd.merge(train, recent_app, on='SK_ID_CURR')\ntest = pd.merge(test, recent_app, on='SK_ID_CURR')\n\nrecent_app.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for each ID, average values for each features in previous applications\nprev_app_mean = previous_application.groupby('SK_ID_CURR', as_index=False).mean()\nprev_app_mean.drop(['SK_ID_PREV'], axis=1, inplace=True)\n\n#prefix addition\nprev_app_mean.columns = ['PREV_' + col_name + '_MEAN' if col_name != 'SK_ID_CURR' else col_name for col_name in prev_app_mean.columns]\n\n#merge to train and test dataset\ntrain = pd.merge(train, prev_app_mean, on='SK_ID_CURR')\ntest = pd.merge(test, prev_app_mean, on='SK_ID_CURR')\n\nprev_app_mean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Installment Payment Dataset\n### - Feature Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for each ID, count the number of installment payment\ninst_pay_count = installment_payment[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR', as_index=False).count()\ninst_pay_count.rename(columns={'SK_ID_PREV':'INST_PAY_COUNT'}, inplace=True)\n\n#merge to train and test dataset\ntrain = pd.merge(train, inst_pay_count, on='SK_ID_CURR')\ntest = pd.merge(test, inst_pay_count, on='SK_ID_CURR')\n\ninst_pay_count.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for each ID, average values for each features in installment payment\ninst_pay_mean = installment_payment.groupby('SK_ID_CURR', as_index=False).mean()\ninst_pay_mean.drop(['SK_ID_PREV'], axis=1, inplace=True)\n\n#prefix addition\ninst_pay_mean.columns = ['INST_' + col_name + '_MEAN' if col_name != 'SK_ID_CURR' else col_name for col_name in inst_pay_mean.columns]\n\n#merge to train and test dataset\ntrain = pd.merge(train, inst_pay_mean, on='SK_ID_CURR')\ntest = pd.merge(test, inst_pay_mean, on='SK_ID_CURR')\n\ninst_pay_mean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Test Dataset\n### - Feature Creation\n- DEBT_BURDEN_RATIO : The ratio of the debts you have to your average monthly income. Let's say the lenders set DBR threshold to 35%\n- ANNUITY_TO_DBR : Percentage of annuity to Debt Burden Ratio\n- ANNUITY_TO_CREDIT : Percentage of annuity to approved credit"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_dataset = [train, test] \n\nfor dataset in main_dataset:\n    dataset['DEBT_BURDEN_RATIO'] = dataset['AMT_INCOME_TOTAL'] * (35/100)\n    dataset['ANNUITY_TO_DBR'] = (dataset['AMT_ANNUITY'] / dataset['DEBT_BURDEN_RATIO']) * 100\n    dataset['ANNUITY_TO_CREDIT'] = (dataset['AMT_ANNUITY'] / dataset['AMT_CREDIT']) * 100\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Correlations\nFinding correlations of all features with the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlations\ntrain.corr()['TARGET'].sort_values(ascending=False)[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop features based on correlations\nfeatures_to_be_dropped = ['SK_ID_CURR',\n                          #'TARGET',\n                          'FLAG_MOBIL']\n\n#store target array\ntarget_array = train['TARGET']\ntrain.drop('TARGET', axis=1, inplace=True)\n#target_array_train = train['TARGET']\n#target_array_test = test['TARGET']\n\n#store test's LN_ID\nSK_ID_CURR = test['SK_ID_CURR']\n\n#drop features\ntrain.drop(features_to_be_dropped, axis=1, inplace=True)\ntest.drop(features_to_be_dropped, axis=1, inplace=True)\n\n#print shape\nprint('train shape: ', train.shape)\nprint('test shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - One-Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SOURCE'] = 'train'\ntest['SOURCE'] = 'test'\n\n#combine train and test dataset\ncombined_data = pd.concat([train, test], ignore_index=True)\n\nprint(train.shape, test.shape, combined_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create dummies\ncombined_data = pd.get_dummies(combined_data, drop_first=True)\ncombined_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating features matrix (X) and target array (y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = combined_data[combined_data['SOURCE_train'] == 1].copy()\ny = target_array\n\nX_test = combined_data[combined_data['SOURCE_train'] == 0].copy()\n\nX.drop(['SOURCE_train'], axis=1, inplace=True)\nX_test.drop(['SOURCE_train'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Check For Overfitting\nCheck for overfitting caused by redundant zeroes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def overfit_zeros(df, limit=99.95):\n    \"\"\"df (dataframe)  : data\n       limit (float)   : limit to be called overfitted\n       Returns a list of features that have redundant zeroes and caused overfitting.\n    \"\"\"\n    overfit = []\n    \n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros / len(df) * 100 > limit:\n            overfit.append(i)\n            \n    overfit = list(overfit)\n    \n    return overfit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of overfitted features\noverfitted_features = overfit_zeros(X)\n\n#print overfitted features\nprint('Overfitted features :')\nprint('---------------------')\nfor feature in overfitted_features:\n    print(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop overfitted features\nX.drop(overfitted_features, axis=1, inplace=True)\nX_test.drop(overfitted_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating A Model\n## Model Selection: CatBoostClassifier\nCatBoost name comes from two words, Category and Boosting. Boost comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well.\n\nWe begin by splitting data into two subsets: for training data and for validating data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n#tuning the model\ncat_boost = CatBoostClassifier(iterations = 1000,\n                               scale_pos_weight = 11, #from the ratio between majority class to minority class\n                               learning_rate = 0.01, \n                               depth = 8,\n                               eval_metric = 'AUC',\n                               random_seed = 7)\n\n#fitting\ncat_boost.fit(X_train, y_train, \n              eval_set=(X_eval, y_eval))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics\nThe results will be on evaluated on area under the ROC (Receiver Operating Characteristic) curve between the predicted probability and the observed target, because we deal with imbalanced dataset. We should not use accuracy score beacuse it will be bias to majority class.\n\nThe Area Under the ROC Curve, known as ROC AUC, measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1). AUC ranges in value from 0 to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n#probability\ncat_boost_positive_prob = cat_boost.predict_proba(X_eval)[:, 1]\n\nprint('CatBoostClassifier ROC AUC score : ', roc_auc_score(y_eval, cat_boost_positive_prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances\n```feature_importances_``` attribute gives us a list where the higher score the more important that feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature importances\nfeature_importances = pd.DataFrame({'feature'   : X_train.columns,\n                                    'importance': cat_boost.feature_importances_})\n\n#plot top 10 feature importances\nsns.barplot(x='importance', y='feature', data=feature_importances.sort_values('importance', ascending=False)[:10]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make A Prediction\nIn this section, we will make a predicted probability for positive class, i.e. client had payment difficulties, and sorted/ranked them from the highest probability to the lowest. From there, the lenders can set a threshold to determine how much of the potential risk that can be accepted and choose who are eligible for a loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a predicted probability dataframe\nprediction = pd.DataFrame({'SK_ID_CURR': SK_ID_CURR,\n                           'TARGET_POSITIVE_PROB': cat_boost.predict_proba(X_test)[:, 1]})\n\n#sort/ranked from the highest probability to the lowest\nprediction = prediction.sort_values('TARGET_POSITIVE_PROB', ascending=False)\n\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}